{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilize the dataset that is provided to make a NBA predicition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import python package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_elo = 1600\n",
    "team_elos = {}\n",
    "team_stats = {}\n",
    "X = []\n",
    "y = []\n",
    "folder = 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define some helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate Elo score\n",
    "def calc_elo(win_team, lose_team):\n",
    "    winner_rank = get_elo(win_team)\n",
    "    loser_rank = get_elo(lose_team)\n",
    "\n",
    "    rank_diff = winner_rank - loser_rank\n",
    "    exp = (rank_diff * -1) / 400\n",
    "    odds = 1 / (1 + math.pow(10, exp))\n",
    "    if winner_rank < 2100:\n",
    "        k = 32\n",
    "    elif 2100 <= winner_rank < 2400:\n",
    "        k = 24\n",
    "    else:\n",
    "        k = 16\n",
    "    new_winner_rank = round(winner_rank + (k * (1 - odds)))\n",
    "    new_rank_diff = new_winner_rank - winner_rank\n",
    "    new_loser_rank = loser_rank - new_rank_diff\n",
    "\n",
    "    return new_winner_rank, new_loser_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv file and build information matrix\n",
    "def initialize_data(Mstat, Ostat, Tstat):\n",
    "    new_Mstat = Mstat.drop(['Rk', 'Arena'], axis=1)\n",
    "    new_Ostat = Ostat.drop(['Rk', 'G', 'MP'], axis=1)\n",
    "    new_Tstat = Tstat.drop(['Rk', 'G', 'MP'], axis=1)\n",
    "\n",
    "    team_stats1 = pd.merge(new_Mstat, new_Ostat, how='left', on='Team')\n",
    "    team_stats1 = pd.merge(team_stats1, new_Tstat, how='left', on='Team')\n",
    "\n",
    "    # print(team_stats1.info())\n",
    "    return team_stats1.set_index('Team', inplace=False, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elo(team):\n",
    "    try:\n",
    "        return team_elos[team]\n",
    "    except:\n",
    "        # if not, init elo score as base_elo\n",
    "        team_elos[team] = base_elo\n",
    "        return team_elos[team]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataSet(team_stats, all_data):\n",
    "    # print(\"Building data set..\")\n",
    "    for index, row in all_data.iterrows():\n",
    "        WLoc = ''\n",
    "        if float(row['PTS']) > float(row['PTS.1']):\n",
    "            Wteam = row['Visitor/Neutral']\n",
    "            Lteam = row['Home/Neutral']\n",
    "            WLoc = 'V'\n",
    "        else:\n",
    "            Wteam = row['Home/Neutral']\n",
    "            Lteam = row['Visitor/Neutral']\n",
    "            WLoc = 'H'\n",
    "\n",
    "        team1_elo = get_elo(Wteam)\n",
    "        team2_elo = get_elo(Lteam)\n",
    "\n",
    "        if WLoc == 'H':\n",
    "            team1_elo += 100\n",
    "        else:\n",
    "            team2_elo += 100\n",
    "\n",
    "        team1_features = [team1_elo]\n",
    "        team2_features = [team2_elo]\n",
    "\n",
    "        for key, value in team_stats.loc[Wteam].iteritems():\n",
    "            team1_features.append(value)\n",
    "        for key, value in team_stats.loc[Lteam].iteritems():\n",
    "            team2_features.append(value)\n",
    "\n",
    "        if WLoc=='H':\n",
    "            X.append(team1_features + team2_features)\n",
    "            y.append(1)\n",
    "        else:\n",
    "            X.append(team2_features + team1_features)\n",
    "            y.append(0)\n",
    "\n",
    "        new_winner_rank, new_loser_rank = calc_elo(Wteam, Lteam)\n",
    "        team_elos[Wteam] = new_winner_rank\n",
    "        team_elos[Lteam] = new_loser_rank\n",
    "\n",
    "    return np.nan_to_num(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load features' files. In this sample, information of 2018-19 season is loaded. If info of other season should be loaded, just modify `18-19` to the season you want, such as `15-16`, `16-17` or `17-18`. But .csv file should be storaged in `folder`(./data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mstat = pd.read_csv(folder + '/18-19Miscellaneous_Stat.csv')\n",
    "Ostat = pd.read_csv(folder + '/18-19Opponent_Per_Game_Stat.csv')\n",
    "Tstat = pd.read_csv(folder + '/18-19Team_Per_Game_Stat.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "re-format features to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_stats = initialize_data(Mstat, Ostat, Tstat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load match infos as label. In this case, trainning set and test set of match result is collected from `Year_2018_2019.csv`. Files,`Year_2016_2017.csv` and `Year_2017_2018.csv`, are open to those code by shift code of `result_data = pd.read_csv(folder + '/Year_2018_2019.csv')` to:\n",
    "\n",
    "**result_data = pd.read_csv(folder + '/Year_2016_2017.csv')** or \n",
    "\n",
    "**result_data = pd.read_csv(folder + '/Year_2017_2018.csv')**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data = pd.read_csv(folder + '/Year_2018_2019.csv')\n",
    "X, y = build_dataSet(team_stats, result_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's Check information about features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape= (1312, 136)\n",
      "Which means there are 1312 samples, each sample has 136 features.\n",
      "such as:Feature\n",
      "      0     1     2     3     4     5     6     7     8      9    ...    \\\n",
      "0  1683.0  26.2  36.0  46.0  37.0  45.0 -1.50  0.48 -1.02  111.4  ...     \n",
      "1  1821.0  24.9  54.0  28.0  51.0  31.0  3.95  0.24  4.19  113.0  ...     \n",
      "2  1737.0  25.7  49.0  33.0  52.0  30.0  4.44 -0.54  3.90  112.2  ...     \n",
      "3  1941.0  28.4  57.0  25.0  56.0  26.0  6.46 -0.04  6.42  115.9  ...     \n",
      "4  1656.0  25.7  33.0  49.0  38.0  44.0 -1.33  0.23 -1.10  111.4  ...     \n",
      "5  1724.0  26.2  36.0  46.0  37.0  45.0 -1.50  0.48 -1.02  111.4  ...     \n",
      "6  1707.0  26.9  33.0  49.0  38.0  44.0 -1.28  0.42 -0.87  109.4  ...     \n",
      "7  1734.0  25.7  49.0  33.0  50.0  32.0  3.40  0.15  3.56  110.3  ...     \n",
      "8  1883.0  29.2  53.0  29.0  53.0  29.0  4.77  0.19  4.96  115.5  ...     \n",
      "9  1655.0  25.7  49.0  33.0  50.0  32.0  3.40  0.15  3.56  110.3  ...     \n",
      "\n",
      "     126   127   128   129   130  131  132   133   134    135  \n",
      "0  0.779   9.1  31.3  40.4  23.9  9.0  5.1  15.6  23.6  107.5  \n",
      "1  0.747  11.4  33.6  45.0  22.5  6.9  4.0  13.8  22.1  107.0  \n",
      "2  0.752   9.3  33.7  43.0  26.0  8.7  4.9  13.7  19.4  108.0  \n",
      "3  0.804   9.6  35.6  45.2  25.4  8.3  5.3  14.0  21.0  114.4  \n",
      "4  0.772   8.8  33.0  41.8  23.9  8.3  5.5  14.0  22.0  103.5  \n",
      "5  0.797   9.9  33.9  43.8  23.2  7.2  4.9  12.2  18.9  110.7  \n",
      "6  0.802   9.8  34.7  44.5  26.3  8.6  5.3  12.8  20.4  112.4  \n",
      "7  0.759  10.5  34.3  44.7  20.1  6.8  5.1  14.0  20.9  104.6  \n",
      "8  0.736  10.0  36.4  46.4  26.0  8.1  5.9  15.1  21.1  111.7  \n",
      "9  0.792   9.7  35.8  45.5  24.0  6.8  4.7  14.5  23.3  115.1  \n",
      "\n",
      "[10 rows x 136 columns]\n",
      "lables:(1 denotes HomeWin, 0 denotes VisitorWin)\n",
      "   0\n",
      "0  1\n",
      "1  1\n",
      "2  1\n",
      "3  0\n",
      "4  1\n",
      "5  1\n",
      "6  1\n",
      "7  1\n",
      "8  1\n",
      "9  1\n"
     ]
    }
   ],
   "source": [
    "print('X.shape=',X.shape)\n",
    "print('Which means there are {} samples, each sample has {} features.'.format(X.shape[0],X.shape[1]))\n",
    "print('such as:Feature')\n",
    "print(pd.DataFrame(X).head(10))\n",
    "print('lables:(1 denotes HomeWin, 0 denotes VisitorWin)')\n",
    "print(pd.DataFrame(y).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And normally in machine learning, data sets should be shuffled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "per = np.random.permutation(len(y))\n",
    "X = X[per, :]\n",
    "y = y[per]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data sets to training set and test set. 80% of data should be for training, and remainder for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample of trainning set:1049,sample of test set:263\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print('sample of trainning set:{},sample of test set:{}'.format(len(y_train), len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build Decision Tree Random Forest XGBoost Logistic Regression Naive Bayes (Gaussian)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and train **LogisticRegression** model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score of Logistic Regression model in test set is 0.7034 (score of training set:0.6997)\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(penalty='l2', tol=0.00001, C=0.5, solver='liblinear')\n",
    "lr.fit(X_train, y_train)\n",
    "score_lr = lr.score(X_test, y_test)\n",
    "score_lr_train = lr.score(X_train, y_train)\n",
    "print('score of Logistic Regression model in test set is {:.4f} (score of training set:{:.4f})'.format(score_lr,score_lr_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and train **GaussianNB** model. \n",
    "more setting:https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score of Naive Bayes (Gaussian) model in test set is 0.6654 (score of training set:0.6692)\n"
     ]
    }
   ],
   "source": [
    "gnb = GaussianNB(var_smoothing=1e-09)\n",
    "gnb.fit(X_train, y_train)\n",
    "score_gnb = gnb.score(X_test, y_test)\n",
    "score_gnb_train = gnb.score(X_train, y_train)\n",
    "print('score of Naive Bayes (Gaussian) model in test set is {:.4f} (score of training set:{:.4f})'.format(score_gnb,score_gnb_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and train **Random Forest** model.\n",
    "more setting:https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score of Random Forest model in test set is 0.6122 (score of training set:0.9762)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "score_rf = rf.score(X_test, y_test)\n",
    "score_rf_train = rf.score(X_train, y_train)\n",
    "print('score of Random Forest model in test set is {:.4f} (score of training set:{:.4f})'.format(score_rf,score_rf_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and train **Decision Tree** model.\n",
    "more setting: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score of Decision Tree model in test set is 0.5513 (score of training set:0.9990)\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "score_dt = dt.score(X_test, y_test)\n",
    "score_dt_train = dt.score(X_train, y_train)\n",
    "print('score of Decision Tree model in test set is {:.4f} (score of training set:{:.4f})'.format(score_dt,score_dt_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and train **XGBoost** model. \n",
    "more setting: https://dask-ml.readthedocs.io/en/stable/modules/generated/dask_ml.xgboost.XGBClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score of XGBoost model in test set is 0.6236 (score of training set:0.8046)\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(learning_rate=0.01, max_depth=5)\n",
    "xgb.fit(X_train, y_train)\n",
    "score_xgb = xgb.score(X_test, y_test)\n",
    "score_xgb_train = xgb.score(X_train, y_train)\n",
    "print('score of XGBoost model in test set is {:.4f} (score of training set:{:.4f})'.format(score_xgb,score_xgb_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For summery info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score of Logistic Regression model in test set is 0.7034 (score of training set:0.6997)\n",
      "score of Naive Bayes (Gaussian) model in test set is 0.6654 (score of training set:0.6692)\n",
      "score of Random Forest model in test set is 0.6122 (score of training set:0.9762)\n",
      "score of Decision Tree model in test set is 0.5513 (score of training set:0.9990)\n",
      "score of XGBoost model in test set is 0.6236 (score of training set:0.8046)\n"
     ]
    }
   ],
   "source": [
    "print('score of Logistic Regression model in test set is {:.4f} (score of training set:{:.4f})'.format(score_lr,score_lr_train))\n",
    "print('score of Naive Bayes (Gaussian) model in test set is {:.4f} (score of training set:{:.4f})'.format(score_gnb,score_gnb_train))\n",
    "print('score of Random Forest model in test set is {:.4f} (score of training set:{:.4f})'.format(score_rf,score_rf_train))\n",
    "print('score of Decision Tree model in test set is {:.4f} (score of training set:{:.4f})'.format(score_dt,score_dt_train))\n",
    "print('score of XGBoost model in test set is {:.4f} (score of training set:{:.4f})'.format(score_xgb,score_xgb_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you wanna test the model with 2019-2020 season match information, an .csv file named `Year_2019_2020.csv` should be storaged in `data` subpath. Re-load match info with code as below.\n",
    "\n",
    "**Note:** if you do not have `Year_2019_2020.csv`, you will get an error as **FileNotFoundError: File b'data/Year_2019_2020.csv' does not exist**\n",
    "\n",
    "If pruning model needed, the websites are listed above for more parameter setting. For example, we create GaussianNB model with one line code `gnb = GaussianNB(var_smoothing=1e-09)`, if the [link](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html) is checked,  two parameter(priors,var_smoothing) can be set. Please feel free to modify one or two parameter(s). That will be interesting if you implyment this part by yourself. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'data/Year_2019_2020.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-74-4da96928f432>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresult_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/Year_2019_2020.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mX_2019\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_2019\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_dataSet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mteam_stats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mscore_lr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_2019\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_2019\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mscore_gnb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgnb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_2019\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_2019\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mscore_rf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_2019\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_2019\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1708\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'data/Year_2019_2020.csv' does not exist"
     ]
    }
   ],
   "source": [
    "result_data = pd.read_csv(folder + '/Year_2019_2020.csv')\n",
    "X_2019, y_2019 = build_dataSet(team_stats, result_data)\n",
    "score_lr = lr.score(X_2019, y_2019)\n",
    "score_gnb = gnb.score(X_2019, y_2019)\n",
    "score_rf = rf.score(X_2019, y_2019)\n",
    "score_dt = dt.score(X_2019, y_2019)\n",
    "score_xgb = xgb.score(X_2019, y_2019)\n",
    "print('score of Logistic Regression model in test set is {:.4f}'.format(score_lr))\n",
    "print('score of Naive Bayes (Gaussian) model in test set is {:.4f}'.format(score_gnb))\n",
    "print('score of Random Forest model in test set is {:.4f}'.format(score_rf))\n",
    "print('score of Decision Tree model in test set is {:.4f}'.format(score_dt))\n",
    "print('score of XGBoost model in test set is {:.4f}'.format(score_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
